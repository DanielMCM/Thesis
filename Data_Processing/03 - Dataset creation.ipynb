{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import DataFrameClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates \n",
    "import requests \n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import ast\n",
    "import zlib\n",
    "import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Order Book merge functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVENT_TIME (E)\n",
    "def create_Binance(part, part_dif, last_row):\n",
    "    # Pass numbers to float format\n",
    "    part_dif.at[:,\"Seg\"] = part_dif.loc[:,\"Seg\"].apply(lambda x: ast.literal_eval(x))\n",
    "    part.at[:, \"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "    part.at[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "    part_dif.at[:, \"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "    part_dif.at[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "\n",
    "    part[\"Type\"] = \"Snapshot\"\n",
    "    part_dif[\"Type\"] = \"Dif\"\n",
    "    if type(last_row) == type(None):\n",
    "        # Gather first snapshot valid\n",
    "        first = 0\n",
    "        for i in range(len(part)):\n",
    "            for j in range(len(part_dif)):\n",
    "                if int(part_dif.iloc[j,4][1]) <= int(part.iloc[i,3]) and int(part_dif.iloc[j,4][0]) > int(part.iloc[i,3]):\n",
    "                    first = [i,j]\n",
    "                    break\n",
    "                elif int(part_dif.iloc[j,4][1]) > int(part.iloc[i,3]):\n",
    "                    break\n",
    "            if first != 0:\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        first = [0,0] \n",
    "\n",
    "    # Add rest of snapshots to the dataset\n",
    "    part_dif.at[:,\"U_Asks\"] = \"\"\n",
    "    part_dif.at[:,\"U_Bids\"] = \"\"\n",
    "\n",
    "    firsts = [element[1] for element in part_dif.iloc[1::,4]]\n",
    "    seconds = [element[0] for element in part_dif.iloc[1::,4]]\n",
    "\n",
    "    pos = []\n",
    "\n",
    "    df = part\n",
    "    for element in df.iloc[:,3]:\n",
    "        pos.append(next(x[0] for x in enumerate(firsts) if x[1] >=  int(element)))\n",
    "        i = 0\n",
    "        for element in pos:\n",
    "            if seconds[pos[i]] >= int(df.iloc[i,3]):\n",
    "                part_dif.iat[element,6] = df.iloc[i,0]\n",
    "                part_dif.iat[element,7] = df.iloc[i,1]\n",
    "            i += 1\n",
    "    # Rebuild order book for each timestemp we have\n",
    "\n",
    "    # Start from first snapshot\n",
    "    part_join = part_dif.iloc[first[1]::,[0,1,4,5,6,7]]\n",
    "    if type(last_row) != type(None):\n",
    "        part_join = pd.concat([pd.DataFrame(last_row).T, part_join])\n",
    "\n",
    "    # Rebuild with differences, keeping snapshots when possible\n",
    "    for i in range(len(part_join)):\n",
    "        for j in range(2):\n",
    "            if part_join.iloc[i,4+j] == \"\":\n",
    "                part_join.iat[i,4+j] = part_join.iloc[i-1,4+j].copy()\n",
    "                if i > 1:\n",
    "                    if part_join.iloc[i,2][1] != part_join.iloc[i-1,2][0] + 1:\n",
    "                        print(\"\\n BINANCE ERROR\")\n",
    "                        print(\"Missing records between \" + str(part_join.iloc[i-1,2][0]) + \" and \" + str(part_join.iloc[i,2][1]) + \"\\n\")\n",
    "            for key in part_join.iloc[i,j].keys():\n",
    "                if  key in part_join.iloc[i,4+j]:\n",
    "                    if part_join.iloc[i,j][key] == 0.0:\n",
    "                        del part_join.iloc[i,4+j][key]\n",
    "                    else:\n",
    "                        part_join.iat[i,4+j][key] = part_join.iloc[i,j][key]\n",
    "                elif part_join.iloc[i,j][key] != 0.0:\n",
    "                    part_join.iat[i,4+j][key] = part_join.iloc[i,j][key]\n",
    "    lrow = part_join.iloc[-1,:].copy()  \n",
    "\n",
    "    return part_join.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Bitfinex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Bitfinex(part, part_dif, last_row):\n",
    "    \n",
    "    part.at[:,\"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[2]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.at[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[2]] for i in ast.literal_eval(x)]) if x != \"-\" else x)  \n",
    "    part_dif.at[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: float(x)) \n",
    "    part_dif.at[:,\"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x: float(x)) \n",
    "    part_dif.at[:,\"Seg\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: int(x)) \n",
    "\n",
    "    join_part = pd.concat([part[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]], part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\",\"Seg\"]]]).sort_index()\n",
    "    if type(last_row) != type(None):\n",
    "        join_part = pd.concat([pd.DataFrame(last_row).T, join_part])\n",
    "    join_part.at[:,\"U_Asks\"] = \"\"\n",
    "    join_part.at[:,\"U_Bids\"] = \"\"\n",
    "    join_part.iat[0,5] = part.iloc[0,0].copy()\n",
    "    join_part.iat[0,6] = part.iloc[0,1].copy()\n",
    "    \n",
    "    current = join_part.iloc[0,3]\n",
    "    for i in range(1,len(join_part)):\n",
    "        join_part.iat[i,5] = join_part.iloc[i-1,5].copy()\n",
    "        join_part.iat[i,6] = join_part.iloc[i-1,6].copy()\n",
    "        if type(join_part.iloc[i,1]) != type({}) and join_part.iloc[i-1,3] == current:\n",
    "            if join_part.iloc[i,0] < 0:\n",
    "                if  join_part.iloc[i,1] in join_part.iloc[i,5]:\n",
    "                    if join_part.iloc[i,4] == 0 and join_part.iloc[i,0] == -1:\n",
    "                        del join_part.iloc[i,5][join_part.iloc[i,1]]\n",
    "                    else:\n",
    "                        join_part.iat[i,5][join_part.iloc[i,1]] = float(join_part.iloc[i,0])\n",
    "                else:\n",
    "                    join_part.iat[i,5][join_part.iloc[i,1]] = join_part.iloc[i,0]\n",
    "            if join_part.iloc[i,0] > 0:\n",
    "                if  join_part.iloc[i,1] in join_part.iloc[i,6] and join_part.iloc[i,0] == 1:\n",
    "                    if join_part.iloc[i,4] == 0:\n",
    "                        del join_part.iloc[i,6][join_part.iloc[i,1]]\n",
    "                    else:\n",
    "                        join_part.iat[i,6][join_part.iloc[i,1]] = join_part.iloc[i,0]\n",
    "                else:\n",
    "                    join_part.iat[i,6][join_part.iloc[i,1]] = join_part.iloc[i,0]\n",
    "        elif type(join_part.iloc[i,1]) == type({}):\n",
    "            join_part.iat[i,5] = join_part.iloc[i,0].copy()\n",
    "            join_part.iat[i,6] = join_part.iloc[i,1].copy()\n",
    "            current = join_part.iloc[0,3]\n",
    "        else:\n",
    "            join_part.iat[i,5] = join_part.iloc[i-1,5].copy()\n",
    "            join_part.iat[i,6] = join_part.iloc[i-1,6].copy()\n",
    "    \n",
    "    join_part.at[:,\"U_Asks\"] = join_part[\"U_Asks\"].apply(lambda x: dict([[str(key), np.abs(x[key])] for key in x.keys()]))\n",
    "    lrow = join_part.iloc[-1,:].copy()\n",
    "    return join_part.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Bithumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in msg\n",
    "def create_Bithumb(part, part_dif, last_row):\n",
    "    # Evaluate strings\n",
    "    part_dif.at[:,\"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x:[[element[0],float(element[1])] for element in ast.literal_eval(x)] if x != \"-\" else x)\n",
    "    part_dif.at[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x:[[element[0],float(element[1])] for element in ast.literal_eval(x)] if x != \"-\" else x)\n",
    "    part_dif = part_dif[part_dif[\"Pair\"] == \"ethbtc\"].copy()\n",
    "\n",
    "    part_dif.at[:,\"Number\"] = part_dif.loc[:,\"Seg\"].apply(lambda x: int(ast.literal_eval(x)[0]))\n",
    "\n",
    "    part_dif = part_dif.sort_values(by = [\"Number\"])\n",
    "\n",
    "    part_dif.at[:,\"NextNumber\"] = part_dif.loc[:,\"Seg\"].shift(-1).fillna(\"[0]\").apply(lambda x: int(ast.literal_eval(x)[0]))\n",
    "\n",
    "    if len(part_dif[(part_dif[\"Number\"] + 1) != part_dif[\"NextNumber\"]]) > 1:\n",
    "        print(\"Records missing Bithumb!\")\n",
    "        print(part_dif[(part_dif[\"Number\"] + 1) != part_dif[\"NextNumber\"]])\n",
    "\n",
    "    part.at[:,\"Asks\"] =  part.loc[:,\"Asks\"].apply(lambda x: dict([[element[0],float(element[1])] for element in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.at[:,\"Bids\"] =  part.loc[:,\"Bids\"].apply(lambda x: dict([[element[0],float(element[1])] for element in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "\n",
    "\n",
    "\n",
    "    part.at[:,\"Type\"] = \"Snapshot\"\n",
    "    part_dif.at[:,\"Type\"] = \"Dif\"    \n",
    "\n",
    "    # Create updated order book\n",
    "    if len(part) != 0:  \n",
    "        part_dif = pd.concat([part.loc[:,(\"Asks\",\"Bids\", \"Host\", \"Pair\", \"Type\")], part_dif.loc[:,(\"Asks\",\"Bids\", \"Host\", \"Pair\", \"Type\")]]).sort_index()\n",
    "    else:\n",
    "        part_dif = part_dif.loc[:,(\"Asks\",\"Bids\", \"Host\", \"Pair\", \"Type\")]\n",
    "\n",
    "    part_dif.at[:,\"U_Asks\"] = \"\"\n",
    "    part_dif.at[:,\"U_Bids\"] = \"\"\n",
    "\n",
    "    if type(last_row) != type(None):\n",
    "        part_dif = pd.concat([pd.DataFrame(last_row).T, part_dif])  \n",
    "        part_dif.iat[0,4] = \"Previous\"   \n",
    "\n",
    "    for i in range(len(part_dif)):\n",
    "        if part_dif.iloc[i,4] == \"Snapshot\":\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i,0].copy()\n",
    "            part_dif.iat[i,6] = part_dif.iloc[i,1].copy()\n",
    "        elif part_dif.iloc[i,4] == \"Dif\":            \n",
    "            part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "            part_dif.iat[i,6] = part_dif.iloc[i-1,6].copy()\n",
    "            for element in part_dif.iat[i,0]:\n",
    "                if element != \"[]\":\n",
    "                    if element[0] in part_dif.iloc[i,5] and (element[1] == '0.000000' or element[1] == 0):\n",
    "                        del part_dif.iloc[i,5][element[0]]\n",
    "                    else:\n",
    "                        part_dif.iat[i,5][element[0]] = element[1]\n",
    "            for element in part_dif.iat[i,1]:\n",
    "                if element != \"[]\":\n",
    "                    if element[0] in part_dif.iloc[i,6] and (element[1] == '0.000000' or element[1] == 0):\n",
    "                        del part_dif.iloc[i,6][element[0]]\n",
    "                    else:\n",
    "                        part_dif.iat[i,6][element[0]] = element[1]\n",
    "\n",
    "    lrow = part_dif.iloc[-1,:].copy()\n",
    "    return part_dif.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Bitstamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "def create_Bitstamp(part, part_dif):\n",
    "    # First updates\n",
    "\n",
    "    # Evaluate strings\n",
    "    part.at[:,\"Asks\"] =  part.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.at[:,\"Bids\"] =  part.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    \n",
    "    # Create updated order book\n",
    "\n",
    "    part_join = part.rename(columns = {\"Asks\":\"U_Asks\", \"Bids\":\"U_Bids\"})\n",
    "    return part_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Coinbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "def create_Coinbase(part, part_dif, last_row):\n",
    "\n",
    "    # Evaluate strings\n",
    "    part_dif.at[:,\"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x: ast.literal_eval(x) if x != \"-\" else x)\n",
    "    part_dif.at[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: ast.literal_eval(x) if x != \"-\" else x)\n",
    "    part.at[:,\"Asks\"] =  part.loc[:,\"Asks\"].apply(lambda x:dict(ast.literal_eval(x)) if x != \"-\" else x)\n",
    "    part.at[:,\"Bids\"] =  part.loc[:,\"Bids\"].apply(lambda x: dict(ast.literal_eval(x)) if x != \"-\" else x)\n",
    "    \n",
    "    part_dif = pd.concat([part[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]], part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]]]).sort_index()\n",
    "    \n",
    "    part_dif = part_dif.loc[part_dif[part_dif[\"Asks\"] != \"-\"].index[0]::,:]\n",
    "    #   In case we want to change indexes\n",
    "    # part_dif.index.values[i]\n",
    "    # Rebuild order book for each timestemp we have\n",
    "\n",
    "    part_dif.at[:,\"U_Asks\"] = \"\"\n",
    "    part_dif.at[:,\"U_Bids\"] = \"\"\n",
    "    part_dif.iat[0,4] = part.iloc[0,0].copy()\n",
    "    part_dif.iat[0,5] = part.iloc[0,1].copy()\n",
    "    if type(last_row) != type(None):\n",
    "        part_dif = pd.concat([pd.DataFrame(last_row).T, part_dif])      \n",
    "    current = part_dif.iloc[0,3]\n",
    "    for i in range(1,len(part_dif)):\n",
    "        part_dif.iat[i,4] = part_dif.iloc[i-1,4].copy()\n",
    "        part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "        if part_dif.iloc[i,0] == \"-\" and part_dif.iloc[i-1,3] == current:\n",
    "            for element in part_dif.iloc[i,1]:\n",
    "                if  \"sell\" == element[0]:\n",
    "                    if element[1] in part_dif.iloc[i,4]:\n",
    "                        if element[2] == '0.00000000':\n",
    "                            del part_dif.iloc[i,4][element[1]]\n",
    "                        else:\n",
    "                            part_dif.iat[i,4][element[1]] = element[2]\n",
    "                    elif element[2] != '0.00000000':\n",
    "                        part_dif.iat[i,4][element[1]] = element[2]\n",
    "                else:\n",
    "                    if element[1] in part_dif.iloc[i,5]:\n",
    "                        if element[2] == '0.00000000':\n",
    "                            del part_dif.iloc[i,5][element[1]]\n",
    "                        else:\n",
    "                            part_dif.iat[i,5][element[1]] = element[2]\n",
    "                    elif element[2] != '0.00000000':\n",
    "                        part_dif.iat[i,5][element[1]] = element[2]          \n",
    "        elif part_dif.iloc[i,0] != \"-\":\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i,0].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i,1].copy()\n",
    "            current = part_dif.iloc[0,3]\n",
    "        else:\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i-1,4].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "    \n",
    "    lrow = part_dif.iloc[-1,:].copy()\n",
    "    return part_dif.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Huobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "def create_Huobi(part, part_dif):\n",
    "    # First updates\n",
    "    part.at[:,\"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict(ast.literal_eval(x)) if x != \"-\" else x)\n",
    "    part.at[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict(ast.literal_eval(x)) if x != \"-\" else x)  \n",
    "    part = part.rename(columns = {\"Asks\": \"U_Asks\", \"Bids\": \"U_Bids\"})\n",
    "    return part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 Kraken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "# Careful with \"r\" for republished updates!\n",
    "def create_Kraken(part, part_dif, last_row):\n",
    "    part.at[:,\"Host\"] = \"Snapshot\"\n",
    "    \n",
    "    part_dif = pd.concat([part[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]], part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]]]).sort_index()\n",
    "    part.at[:,\"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.at[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)  \n",
    "    part_dif.at[:,\"Asks\"] =  part_dif.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part_dif.at[:,\"Bids\"] =  part_dif.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)        # Rebuild order book for each timestemp we have\n",
    "\n",
    "    part_dif = part_dif.loc[part_dif[part_dif[\"Host\"] == \"Snapshot\"].index[0]::,:]\n",
    "\n",
    "    part_dif.at[:,\"U_Asks\"] = \"\"\n",
    "    part_dif.at[:,\"U_Bids\"] = \"\"\n",
    "    part_dif.iat[0,4] = part.iloc[0,0].copy()\n",
    "    part_dif.iat[0,5] = part.iloc[0,1].copy()\n",
    "    if type(last_row) != type(None):\n",
    "        part_dif = pd.concat([pd.DataFrame(last_row).T, part_dif])   \n",
    "\n",
    "    current = part_dif.iloc[0,3]\n",
    "    for i in range(1,len(part_dif)):\n",
    "        if part_dif.iloc[i,2] != \"Snapshot\" and part_dif.iloc[i-1,3] == current:\n",
    "            for j in range(2):\n",
    "                part_dif.iat[i,4+j] = part_dif.iloc[i-1,4+j].copy()\n",
    "                if part_dif.iloc[i,j] != \"-\":\n",
    "                    for key in part_dif.iloc[i,j].keys():\n",
    "                        if  key in part_dif.iloc[i,4+j]:\n",
    "                            if float(part_dif.iloc[i,j][key][0]) == 0.0:\n",
    "                                del part_dif.iloc[i,4+j][key]\n",
    "                            else:\n",
    "                                part_dif.iat[i,4+j][key] = part_dif.iloc[i,j][key][0]\n",
    "                        else:\n",
    "                            part_dif.iat[i,4+j][key] = part_dif.iloc[i,j][key][0]        \n",
    "        elif part_dif.iloc[i,2] == \"Snapshot\":\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i,0].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i,1].copy()\n",
    "            current = part_dif.iloc[0,3]\n",
    "        else:\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i-1,4].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "       \n",
    "    lrow = part_dif.iloc[-1,:].copy()\n",
    "    \n",
    "    return part_dif.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(ex, part_dif):\n",
    "    # Compute orderbook price to add to the main dataset\n",
    "    first = 0\n",
    "    second = 0\n",
    "    part_dif.at[:,\"OrderBookPrice \" + ex] = \"\"\n",
    "    indb = part_dif.columns.get_loc(\"U_Bids\")\n",
    "    inda = part_dif.columns.get_loc(\"U_Asks\")\n",
    "    indf = part_dif.columns.get_loc(\"OrderBookPrice \" + ex) \n",
    "    for i in range(len(part_dif)):\n",
    "\n",
    "        values = sorted(part_dif.iloc[i,inda].keys())\n",
    "        values2 = sorted(part_dif.iloc[i,indb].keys(), reverse = True)\n",
    "        for j in values:\n",
    "            if float(part_dif.iloc[i,inda][j]) > 0:\n",
    "                first = float(j)\n",
    "                break\n",
    "        for j in values2:\n",
    "            if float(part_dif.iloc[i,indb][j]) > 0:\n",
    "                second = float(j)\n",
    "                break\n",
    "        if first != 0 and second != 0:\n",
    "            part_dif.iat[i,indf] = (first + second) / 2\n",
    "        elif first != 0 or second != 0:\n",
    "            print(\"ERROR FATAL!!!!\")\n",
    "            try:\n",
    "                print(part_dif.iat[i-1:i+2,:])\n",
    "            except:\n",
    "                pass\n",
    "            part_dif.iat[i,indf] = (first + second)\n",
    "        else:\n",
    "            try:\n",
    "                print(\"ERROR FATAL!!!!\")\n",
    "                print(part_dif.iat[i-1:i+2,:])\n",
    "                part_dif.iat[i,indf] = part_dif.iloc[i-1,indf]\n",
    "            except:\n",
    "                print(\"SUPERERROR FATAL\")\n",
    "                \n",
    "        part_dif.iat[i,indb] = { your_key: part_dif.iloc[i,indb][your_key] for your_key in values2[0:10] }\n",
    "        part_dif.iat[i,inda] = { your_key: part_dif.iloc[i,inda][your_key] for your_key in values[0:10] }\n",
    "\n",
    "    separate = part_dif.iloc[:,[inda,indb]].apply(lambda x: pd.Series([item for sublist in  \n",
    "                                                 [[float(list(x[0].keys())[i]), float(list(x[0].values())[i]),\n",
    "                                                   float(list(x[1].keys())[i]), float(list(x[1].values())[i])] for i in range(10)] \n",
    "                                                  for item in sublist]), axis = 1).rename(columns = dict(\n",
    "                             [[i,\"Price_A_\" + str(int(i/4)) + \"_\" + ex] if i % 4 == 0 else \n",
    "                             [i,\"Price_B_\" + str(int(i/4)) + \"_\" + ex] if i % 4 == 2  else\n",
    "                             [i,\"Quan_A_\" + str(int(i/4)) + \"_\" + ex] if i % 4 == 1 else\n",
    "                             [i,\"Quan_B_\" + str(int(i/4)) + \"_\" + ex] for i in range(40)]))\n",
    "#     [print(col) for col in separate.columns]\n",
    "    part_dif = pd.concat([part_dif.drop([\"U_Asks\", \"U_Bids\"], axis = 1), separate], axis = 1)\n",
    "    \n",
    "    return part_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_main(ex, df2, part_join):\n",
    "    filter_col = [col for col in part_join if (col.startswith('Price_') or col.startswith('Quan_') or col.startswith('OrderBook'))]\n",
    "    df2 = pd.merge(df2,part_join[filter_col], how='outer', left_index=True, right_index=True)\n",
    "    filter_col = [col for col in df2 if (col.startswith('Last') or col.startswith('Order')\n",
    "                                         or col.startswith(\"Price_\") or col.startswith(\"Quan_\"))]\n",
    "    df2.update(df2[filter_col].ffill())\n",
    "    filter_col = [col for col in df2 if (col.startswith('Q ') or col.startswith('Q_'))]\n",
    "    df2.update(df2[filter_col].fillna(0))\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_OrderBook(ex, df2, part,part_dif, last_row = None):\n",
    "    if ex == \"Bitstamp\":\n",
    "        part_dif = create_Bitstamp(part, part_dif)\n",
    "    elif ex == \"Bitfinex\":\n",
    "        part_dif, last_row = create_Bitfinex(part, part_dif, last_row)\n",
    "    elif ex == \"Binance\":\n",
    "        part_dif, last_row = create_Binance(part, part_dif, last_row)\n",
    "    elif ex == \"Huobi\":\n",
    "        part_dif = create_Huobi(part, part_dif)\n",
    "    elif ex == \"Kraken\":\n",
    "        part_dif, last_row = create_Kraken(part, part_dif, last_row)\n",
    "    elif ex == \"Coinbase\":\n",
    "        part_dif, last_row = create_Coinbase(part, part_dif, last_row)\n",
    "    elif ex == \"Bithumb\":\n",
    "        part_dif, last_row = create_Bithumb(part, part_dif, last_row)\n",
    "    \n",
    "    part_dif = compute_stats(ex, part_dif)\n",
    "    df2 = join_main(ex, df2,part_dif)\n",
    "    \n",
    "    return df2, last_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_core(df, pair, exchanges, lrow):\n",
    "    df1 = df[df[\"Pair\"] == pair]\n",
    "    \n",
    "    df2 = df1[df1[\"Host\"] == exchanges[0]].groupby(pd.Grouper(freq = \"ms\")).agg({\"Price\":[np.mean],\"Q\":[np.sum]}).fillna(method = \"ffill\")\n",
    "    df2.columns = df2.columns.droplevel(level = 0)\n",
    "    df2 = df2.rename(columns = {\"mean\": \"LastTrade_\" + exchanges[0], \"sum\": \"Q_\" + exchanges[0]})\n",
    "    for i in exchanges[1::]:\n",
    "        dft = df1[df1[\"Host\"] == i].groupby(pd.Grouper(freq = \"ms\")).agg({\"Price\":[np.mean],\"Q\":[np.sum]}).fillna(method = \"ffill\")\n",
    "        df2[[\"LastTrade_\" + i, \"Q_\"+i]] = dft[[\"Price\", \"Q\"]]\n",
    "        \n",
    "    if type(lrow) != type(None):\n",
    "        df2 = pd.concat([pd.DataFrame(lrow).T, df2])\n",
    "        \n",
    "    col1 = [col for col in df2 if (col.startswith('Last'))]\n",
    "    df2.update(df2[col1].ffill())\n",
    "    col2 = [col for col in df2 if (col.startswith('Q'))]\n",
    "    df2.update(df2[col2].fillna(0))\n",
    "    \n",
    "    df2.drop_duplicates(keep = \"first\", inplace = True)\n",
    "    \n",
    "    return df2.iloc[1::,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(date_init, date_end):\n",
    "    \n",
    "    points = client.query(\"SELECT Host, Pair, Price, Q FROM Price WHERE time >= '\"+ str(date_init) +\"' AND time < '\" + str(date_end) + \"'\")\n",
    "    points_book = client.query(\"SELECT Asks,Bids, Host, LastUpdateID, Pair FROM Book WHERE time >= '\"+ str(date_init) +\"' AND time < '\" + str(date_end) + \"' AND LastUpdateID != ''\")\n",
    "    points_dif = client.query(\"SELECT Asks, Bids, Host, Pair, Seg FROM difBook WHERE time >= '\"+ str(date_init) +\"' AND time < '\" + str(date_end) + \"'\")\n",
    "\n",
    "    last_trades = points[\"Price\"]\n",
    "    book_snap = points_book[\"Book\"]\n",
    "    book_dif = points_dif[\"difBook\"]\n",
    "    \n",
    "    return last_trades,book_snap,book_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_metrics_rmdups(df):\n",
    "    # Compute the max difference and other metrics and remove duplicates\n",
    "    filter_col = [col for col in df if col.startswith('Order')]\n",
    "    df.at[:,\"Maximum\"] = df[filter_col].apply(lambda row: np.max(row) - np.min(row), axis=1)\n",
    "\n",
    "    df.at[:,\"Average\"] = df[filter_col].apply(lambda row: np.mean(row)*0.0025, axis=1)\n",
    "\n",
    "    df.at[:,\"MaxExch\"] = df[filter_col].fillna(0).apply(lambda row: sorted([row.idxmax()[15::], row.idxmin()[15::]])[0], axis=1)\n",
    "    df.at[:,\"MinExch\"] = df[filter_col].fillna(0).apply(lambda row: sorted([row.idxmax()[15::], row.idxmin()[15::]])[1], axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "    col1 = [col for col in df if (col.startswith('Last') or  col.startswith('Order') or col.startswith('Price') or col.startswith('Quan'))]\n",
    "    df.update(df[col1].ffill())\n",
    "    col2 = [col for col in df if (col.startswith('Q ') or col.startswith('Q_'))]\n",
    "    df.update(df[col2].fillna(0))\n",
    "    \n",
    "    # 0.16 is maximum price\n",
    "    filter_col = [col for col in df if col.startswith('Last_Price') or col.startswith('Price')]\n",
    "    df.at[:,filter_col] = df[filter_col] / 0.16\n",
    "    \n",
    "    filter_col = [col for col in df if col.startswith('Q')]\n",
    "    df.at[:,filter_col] = 1/ (1 + df[filter_col])\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_next_0(array):\n",
    "    ones = 0\n",
    "    registers = 0\n",
    "    for j in range(len(array)):\n",
    "        registers += 1\n",
    "        if array[j] == 1:\n",
    "            ones += 1\n",
    "            if ones == 10:\n",
    "                registers = 9\n",
    "                ones = 9\n",
    "        elif ones != 0:\n",
    "            ones = 0\n",
    "            \n",
    "        if registers == 501:\n",
    "            return (j - 500)\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_next_1(array):\n",
    "    ones = 0\n",
    "    pos = 0\n",
    "    for j in range(len(array)):\n",
    "        if array[j] == 1:\n",
    "            if ones == 0:\n",
    "                pos = j\n",
    "            ones += 1\n",
    "            if ones == 10:\n",
    "                return (pos - 490)\n",
    "        else:\n",
    "            ones = 0\n",
    "            \n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_labels(df):\n",
    "    # If there are more than 10 consecutive >avg*0,002 then 1 otherwise 0.\n",
    "    leng = len(df)\n",
    "    \n",
    "    df[\"Label_Not\"] = np.zeros(leng)\n",
    "    df.at[df.iloc[list(np.where(df[\"Average\"]<df[\"Maximum\"])[0].astype(int)),-1].index,\"Label_Not\"] = 1\n",
    "    next10_pos = max(0,calculate_next_1(df[\"Label_Not\"]))\n",
    "    \n",
    "    \n",
    "#     idl = int(df.columns.get_loc(\"Label2\"))\n",
    "    result = []\n",
    "    i = 0\n",
    "    cont = True\n",
    "    if next10_pos == 0:\n",
    "        next_0 = max(0,calculate_next_0(df[\"Label_Not\"]))\n",
    "        run = \"Ones Run\"\n",
    "    else:\n",
    "        run = \"Zeros Run\"\n",
    "        \n",
    "    while cont:\n",
    "              \n",
    "        if run == \"Ones Run\":\n",
    "            result += list(np.ones(next_0))\n",
    "            i = i+next_0\n",
    "            next10_pos = calculate_next_1(df[\"Label_Not\"][i::])\n",
    "            run = \"Zeros Run\"\n",
    "        else:\n",
    "            result += list(np.zeros(next10_pos))\n",
    "            i = i+next10_pos\n",
    "            next_0 = calculate_next_0(df[\"Label_Not\"][i::])\n",
    "            run = \"Ones Run\"\n",
    "\n",
    "        if i >= leng-500:\n",
    "            cont = False\n",
    "            \n",
    "    result += list(np.zeros(leng-len(result)))\n",
    "    \n",
    "    df[\"Label\"] = np.asarray(result)\n",
    "    df = df.drop(\"Label_Not\", axis = 1)\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Final dataframe process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DataFrameClient('localhost', 8086, 'root', 'root')\n",
    "client.switch_database(\"SecondM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.drop_measurement(\"Dataset\")\n",
    "client.get_list_measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Main core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_number = 48\n",
    "interval = 60\n",
    "\n",
    "start_time = datetime.strptime(\"2020-05-31 22:20:00.000000\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "endtime = start_time + timedelta(minutes = 60)\n",
    "\n",
    "finish_process_time = start_time + timedelta(hours = hours_number-1)\n",
    "\n",
    "pairs = [\"ethbtc\"]\n",
    "# pairs = [\"ethbtc\", \"btcusd\", \"ethusd\", \"xtzusd\"]\n",
    "\n",
    "exchanges_snap = [\"Binance\", \"Bitfinex\", \"Bithumb\", \"Bitstamp\", \"Coinbase\", \"Huobi\", \"Kraken\"]\n",
    "\n",
    "analysis_start = None\n",
    "\n",
    "max_dif = 0\n",
    "time_max_dif = 0\n",
    "\n",
    "last_rows = [None]*9\n",
    "\n",
    "matrix_prev = None\n",
    "\n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For every hour\n",
    "start = time.time()\n",
    "i = 0\n",
    "while start_time <= finish_process_time:\n",
    "        \n",
    "    last_trades, book_snap, book_dif = query(start_time, endtime)\n",
    "\n",
    "    # We create the base dataset\n",
    "    core_df = create_core(last_trades, pairs[0],exchanges_snap,last_rows[7])\n",
    "    \n",
    "    last_rows[7] = core_df.iloc[-1,:]\n",
    "    \n",
    "    # We add orderbook data\n",
    "    for k in range(len(exchanges_snap)):\n",
    "#         print(exchanges_snap[k])\n",
    "        part = book_snap[book_snap[\"Host\"] == exchanges_snap[k]]\n",
    "        part = part[part[\"Pair\"].str.contains(pairs[0])].loc[:, part.columns != 'Batch_ID']\n",
    "        \n",
    "        part_dif = book_dif[book_dif[\"Host\"] == exchanges_snap[k]]\n",
    "        part_dif = part_dif[part_dif[\"Pair\"].str.contains(pairs[0])].loc[:, part_dif.columns != 'Batch_ID']\n",
    "\n",
    "        [core_df, last_rows[k]] = create_OrderBook(exchanges_snap[k], core_df, part, part_dif,last_rows[k])\n",
    "\n",
    "    # Add previous row to core\n",
    "    if type(last_rows[8]) != type(None):\n",
    "        core_df = pd.concat([last_rows[8], core_df])\n",
    "    last_rows[8] =  core_df.iloc[-500::,:]  \n",
    "\n",
    "    # Compute last metrics\n",
    "    core_df = last_metrics_rmdups(core_df)\n",
    "\n",
    "    # Check where the maximum difference is\n",
    "    if np.max(core_df[\"Maximum\"]) > max_dif:\n",
    "        max_dif = np.max(core_df[\"Maximum\"]) \n",
    "        time_max_dif = start_time\n",
    "    \n",
    "    #FINISHED CORE CREATION\n",
    "     \n",
    "    \n",
    "    # Generate matrixes\n",
    "    columns = [col for col in core_df if (col.startswith('Price_') or col.startswith('Quan_'))]\n",
    "    columns2 = [col for col in core_df if (col.startswith(\"Last\")  or col.startswith(\"Q_\"))]\n",
    "    matrixes = np.concatenate([np.asarray(core_df[columns2]).reshape(len(core_df), 2,7).transpose(0, 2, 1),\n",
    "                   np.asarray(core_df[columns]).reshape(len(core_df), 7,40)], axis = 2)\n",
    "    \n",
    "    core_df = core_df.drop(columns + columns2, axis = 1)     \n",
    "\n",
    "    # I think because of the notation 1E-5 influxdb read things as string so we convert everything to float\n",
    "    \n",
    "    print(str(datetime.fromtimestamp(time.time())) + \": Writing to dataframe with start time \" + str(start_time))\n",
    "    filter_col = [col for col in core_df if (col.startswith('Maximum') or col.startswith('Ave') or col.startswith('Order'))]\n",
    "    for col in filter_col:\n",
    "        core_df.at[:,col] = core_df.loc[:,col].astype(float)\n",
    "    \n",
    "    # Create Labels\n",
    "    core_df = calculate_labels(core_df)\n",
    "\n",
    "    matrixes = np.abs(matrixes)    \n",
    "    matrixes[matrixes > 1] = 1\n",
    "    matrixes[np.isnan(matrixes)] = 0\n",
    "    \n",
    "    np.save(\"./01_Data/\" + str(i) + \" Input hour \" + str(start_time.year) + \"_\" + str(start_time.month) + \"_\" + str(start_time.day) + \"_\"\n",
    "                                                                + str(start_time.hour) + \"_\" + str(start_time.minute),\n",
    "            matrixes[0:-500])\n",
    "    \n",
    "    np.save(\"./01_Data/\"  + str(i) + \" Labels hour \" + str(start_time.year) + \"_\" + str(start_time.month) + \"_\" + str(start_time.day) + \"_\"\n",
    "                                                                + str(start_time.hour) + \"_\" + str(start_time.minute),\n",
    "            core_df[\"Label\"][0:-500])\n",
    "    \n",
    "    records.append(len(core_df[\"Label\"][0:-500]))\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # Write to database\n",
    "    if type(last_rows[8]) == type(None):\n",
    "       \n",
    "        client.write_points(core_df.iloc[0:-500,:].assign(Batch_ID = [i for i in range(len(core_df)-500)]),\n",
    "                            \"Dataset\",time_precision = \"n\", database = \"SecondM\",tag_columns = ['Batch_ID'], batch_size = 1000)\n",
    "    else:\n",
    "        client.write_points(core_df.iloc[0:-500,:].assign(Batch_ID = [i for i in range(len(core_df)-500)]),\n",
    "                            \"Dataset\",time_precision = \"n\", database = \"SecondM\",tag_columns = ['Batch_ID'], batch_size = 1000)\n",
    "    \n",
    "    # We set up the interval times, and gather the first timestamp that we have data\n",
    "    start_time = endtime\n",
    "    endtime = start_time + timedelta(minutes = interval)\n",
    "    if core_df.isna().sum().sum() == 0 and type(analysis_start) != type(None):\n",
    "        analysis_start = endtime\n",
    "    \n",
    "    print(\"Len Matrix = \" + str(len(matrixes) - 500))\n",
    "    print(\"Len coredf = \" + str(len(core_df) - 500))\n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "with open('len.txt', 'w') as f:\n",
    "    f.write(\"%s\\n\" % records)\n",
    "\n",
    "print(\"Total time: \" + str(end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
