{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import DataFrameClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates \n",
    "import requests \n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Order Book merge functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVENT_TIME (E)\n",
    "def create_Binance(part, part_dif, last_row):\n",
    "    # Pass numbers to float format\n",
    "    part_dif.loc[:,\"Seg\"] = part_dif.loc[:,\"Seg\"].apply(lambda x: ast.literal_eval(x))\n",
    "    part.loc[:, \"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "    part.loc[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "    part_dif.loc[:, \"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "    part_dif.loc[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: dict(list([[i[0],float(i[1])] for i in ast.literal_eval(x)])))\n",
    "\n",
    "    part[\"Type\"] = \"Snapshot\"\n",
    "    part_dif[\"Type\"] = \"Dif\"\n",
    "    if type(last_row) == type(None):\n",
    "        # Gather first snapshot valid\n",
    "        first = 0\n",
    "        for i in range(len(part)):\n",
    "            for j in range(len(part_dif)):\n",
    "                if int(part_dif.iloc[j,4][1]) <= int(part.iloc[i,3]) and int(part_dif.iloc[j,4][0]) > int(part.iloc[i,3]):\n",
    "                    first = [i,j]\n",
    "                    break\n",
    "                elif int(part_dif.iloc[j,4][1]) > int(part.iloc[i,3]):\n",
    "                    break\n",
    "            if first != 0:\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        first = [0,0] \n",
    "\n",
    "    # Add rest of snapshots to the dataset\n",
    "    part_dif[\"U_Asks\"] = \"\"\n",
    "    part_dif[\"U_Bids\"] = \"\"\n",
    "\n",
    "    firsts = [element[1] for element in part_dif.iloc[1::,4]]\n",
    "    seconds = [element[0] for element in part_dif.iloc[1::,4]]\n",
    "\n",
    "    pos = []\n",
    "\n",
    "    df = part\n",
    "    for element in df.iloc[:,3]:\n",
    "        pos.append(next(x[0] for x in enumerate(firsts) if x[1] >=  int(element)))\n",
    "        i = 0\n",
    "        for element in pos:\n",
    "            if seconds[pos[i]] >= int(df.iloc[i,3]):\n",
    "                part_dif.iat[element,6] = df.iloc[i,0]\n",
    "                part_dif.iat[element,7] = df.iloc[i,1]\n",
    "            i += 1\n",
    "    # Rebuild order book for each timestemp we have\n",
    "\n",
    "    # Start from first snapshot\n",
    "    part_join = part_dif.iloc[first[1]::,[0,1,4,5,6,7]]\n",
    "    if type(last_row) != type(None):\n",
    "        part_join = pd.concat([pd.DataFrame(last_row).T, part_join])\n",
    "\n",
    "    # Rebuild with differences, keeping snapshots when possible\n",
    "    for i in range(len(part_join)):\n",
    "        for j in range(2):\n",
    "            if part_join.iloc[i,4+j] == \"\":\n",
    "                part_join.iat[i,4+j] = part_join.iloc[i-1,4+j].copy()\n",
    "                if i > 1:\n",
    "                    if part_join.iloc[i,2][1] != part_join.iloc[i-1,2][0] + 1:\n",
    "                        print(\"\\n BINANCE ERROR\")\n",
    "                        print(\"Missing records between \" + str(part_join.iloc[i-1,2][0]) + \" and \" + str(part_join.iloc[i,2][1]) + \"\\n\")\n",
    "            for key in part_join.iloc[i,j].keys():\n",
    "                if  key in part_join.iloc[i,4+j]:\n",
    "                    if part_join.iloc[i,j][key] == 0.0:\n",
    "                        del part_join.iloc[i,4+j][key]\n",
    "                    else:\n",
    "                        part_join.iat[i,4+j][key] = part_join.iloc[i,j][key]\n",
    "                elif part_join.iloc[i,j][key] != 0.0:\n",
    "                    part_join.iat[i,4+j][key] = part_join.iloc[i,j][key]\n",
    "    lrow = part_join.iloc[-1,:]     \n",
    "\n",
    "    return part_join.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Bitfinex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Bitfinex(part, part_dif, last_row):\n",
    "    \n",
    "    part.loc[:,\"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[2]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.loc[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[2]] for i in ast.literal_eval(x)]) if x != \"-\" else x)  \n",
    "    part_dif.loc[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: float(x)) \n",
    "    part_dif.loc[:,\"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x: float(x)) \n",
    "    part_dif.loc[:,\"Seg\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: int(x)) \n",
    "\n",
    "    join_part = pd.concat([part[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]], part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\",\"Seg\"]]]).sort_index()\n",
    "    if type(last_row) != type(None):\n",
    "        join_part = pd.concat([pd.DataFrame(last_row).T, join_part])\n",
    "    join_part[\"U_Asks\"] = \"\"\n",
    "    join_part[\"U_Bids\"] = \"\"\n",
    "    join_part.iat[0,5] = part.iloc[0,0].copy()\n",
    "    join_part.iat[0,6] = part.iloc[0,1].copy()\n",
    "    \n",
    "    current = join_part.iloc[0,3]\n",
    "    for i in range(1,len(join_part)):\n",
    "        join_part.iat[i,5] = join_part.iloc[i-1,5].copy()\n",
    "        join_part.iat[i,6] = join_part.iloc[i-1,6].copy()\n",
    "        if type(join_part.iloc[i,1]) != type({}) and join_part.iloc[i-1,3] == current:\n",
    "            if join_part.iloc[i,0] < 0:\n",
    "                if  join_part.iloc[i,1] in join_part.iloc[i,5]:\n",
    "                    if join_part.iloc[i,4] == 0 and join_part.iloc[i,0] == -1:\n",
    "                        del join_part.iloc[i,5][join_part.iloc[i,1]]\n",
    "                    else:\n",
    "                        join_part.iat[i,5][join_part.iloc[i,1]] = float(join_part.iloc[i,0])\n",
    "                else:\n",
    "                    join_part.iat[i,5][join_part.iloc[i,1]] = join_part.iloc[i,0]\n",
    "            if join_part.iloc[i,0] > 0:\n",
    "                if  join_part.iloc[i,1] in join_part.iloc[i,6] and join_part.iloc[i,0] == 1:\n",
    "                    if join_part.iloc[i,4] == 0:\n",
    "                        del join_part.iloc[i,6][join_part.iloc[i,1]]\n",
    "                    else:\n",
    "                        join_part.iat[i,6][join_part.iloc[i,1]] = join_part.iloc[i,0]\n",
    "                else:\n",
    "                    join_part.iat[i,6][join_part.iloc[i,1]] = join_part.iloc[i,0]\n",
    "        elif type(join_part.iloc[i,1]) == type({}):\n",
    "            join_part.iat[i,5] = join_part.iloc[i,0].copy()\n",
    "            join_part.iat[i,6] = join_part.iloc[i,1].copy()\n",
    "            current = join_part.iloc[0,3]\n",
    "        else:\n",
    "            join_part.iat[i,5] = join_part.iloc[i-1,5].copy()\n",
    "            join_part.iat[i,6] = join_part.iloc[i-1,6].copy()\n",
    "    \n",
    "    join_part[\"U_Asks\"] = join_part[\"U_Asks\"].apply(lambda x: dict([[str(key), np.abs(x[key])] for key in x.keys()]))\n",
    "    lrow = join_part.iloc[-1,:]\n",
    "    return join_part.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Bithumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in msg\n",
    "def create_Bithumb(part, part_dif, last_row):\n",
    "    # Evaluate strings\n",
    "    part_dif.loc[:,\"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x:[[element[0],float(element[1])] for element in ast.literal_eval(x)] if x != \"-\" else x)\n",
    "    part_dif.loc[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: [[element[0],float(element[1])] for element in ast.literal_eval(x)] if x != \"-\" else x)\n",
    "    part.loc[:,\"Asks\"] =  part.loc[:,\"Asks\"].apply(lambda x: dict([[element[0],float(element[1])] for element in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.loc[:,\"Bids\"] =  part.loc[:,\"Bids\"].apply(lambda x: dict([[element[0],float(element[1])] for element in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part[\"Type\"] = \"Snapshot\"\n",
    "    part_dif[\"Type\"] = \"Dif\"    \n",
    "    \n",
    "    # Create updated order book\n",
    "    if len(part) != 0:  \n",
    "        part_dif = pd.concat([part[[\"Asks\",\"Bids\", \"Host\", \"Pair\", \"Type\"]], part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\", \"Type\"]]]).sort_index()\n",
    "    else:\n",
    "        part_dif = part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\", \"Type\"]]\n",
    "    part_dif[\"U_Asks\"] = \"\"\n",
    "    part_dif[\"U_Bids\"] = \"\"\n",
    "    \n",
    "    if type(last_row) != type(None):\n",
    "        part_dif = pd.concat([pd.DataFrame(last_row).T, part_dif])  \n",
    "        part_dif.iat[0,4] = \"Previous\"   \n",
    "        \n",
    "    for i in range(len(part_dif)):\n",
    "        if part_dif.iloc[i,4] == \"Snapshot\":\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i,0].copy()\n",
    "            part_dif.iat[i,6] = part_dif.iloc[i,1].copy()\n",
    "        elif part_dif.iloc[i,4] == \"Dif\":            \n",
    "            part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "            part_dif.iat[i,6] = part_dif.iloc[i-1,6].copy()\n",
    "            for element in part_dif.iat[i,0]:\n",
    "                if element != \"[]\":\n",
    "                    if element[0] in part_dif.iloc[i,5] and element[1] == '0.000000':\n",
    "                        del part_dif.iloc[i,5][element[0]]\n",
    "                    else:\n",
    "                        part_dif.iat[i,5][element[0]] = element[1]\n",
    "            for element in part_dif.iat[i,1]:\n",
    "                if element != \"[]\":\n",
    "                    if element[0] in part_dif.iloc[i,6] and element[1] == '0.000000':\n",
    "                        del part_dif.iloc[i,6][element[0]]\n",
    "                    else:\n",
    "                        part_dif.iat[i,6][element[0]] = element[1]\n",
    "    lrow = part_dif.iloc[-1,:] \n",
    "    return part_dif.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Bitstamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "def create_Bitstamp(part, part_dif):\n",
    "    # First updates\n",
    "\n",
    "    # Evaluate strings\n",
    "    part.loc[:,\"Asks\"] =  part.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.loc[:,\"Bids\"] =  part.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    \n",
    "    # Create updated order book\n",
    "\n",
    "    part_join = part.rename(columns = {\"Asks\":\"U_Asks\", \"Bids\":\"U_Bids\"})\n",
    "    return part_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Coinbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "def create_Coinbase(part, part_dif, last_row):\n",
    "\n",
    "    # Evaluate strings\n",
    "    part_dif.loc[:,\"Asks\"] = part_dif.loc[:,\"Asks\"].apply(lambda x: ast.literal_eval(x) if x != \"-\" else x)\n",
    "    part_dif.loc[:,\"Bids\"] = part_dif.loc[:,\"Bids\"].apply(lambda x: ast.literal_eval(x) if x != \"-\" else x)\n",
    "    part.loc[:,\"Asks\"] =  part.loc[:,\"Asks\"].apply(lambda x:dict(ast.literal_eval(x)) if x != \"-\" else x)\n",
    "    part.loc[:,\"Bids\"] =  part.loc[:,\"Bids\"].apply(lambda x: dict(ast.literal_eval(x)) if x != \"-\" else x)\n",
    "    \n",
    "    part_dif = pd.concat([part[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]], part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]]]).sort_index()\n",
    "    \n",
    "    part_dif = part_dif.loc[part_dif[part_dif[\"Asks\"] != \"-\"].index[0]::,:]\n",
    "    #   In case we want to change indexes\n",
    "    # part_dif.index.values[i]\n",
    "    # Rebuild order book for each timestemp we have\n",
    "\n",
    "    part_dif[\"U_Asks\"] = \"\"\n",
    "    part_dif[\"U_Bids\"] = \"\"\n",
    "    part_dif.iat[0,4] = part.iloc[0,0].copy()\n",
    "    part_dif.iat[0,5] = part.iloc[0,1].copy()\n",
    "    if type(last_row) != type(None):\n",
    "        part_dif = pd.concat([pd.DataFrame(last_row).T, part_dif])      \n",
    "    current = part_dif.iloc[0,3]\n",
    "    for i in range(1,len(part_dif)):\n",
    "        part_dif.iat[i,4] = part_dif.iloc[i-1,4].copy()\n",
    "        part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "        if part_dif.iloc[i,0] == \"-\" and part_dif.iloc[i-1,3] == current:\n",
    "            for element in part_dif.iloc[i,1]:\n",
    "                if  \"sell\" == element[0]:\n",
    "                    if element[1] in part_dif.iloc[i,4]:\n",
    "                        if element[2] == '0.00000000':\n",
    "                            del part_dif.iloc[i,4][element[1]]\n",
    "                        else:\n",
    "                            part_dif.iat[i,4][element[1]] = element[2]\n",
    "                    elif element[2] != '0.00000000':\n",
    "                        part_dif.iat[i,4][element[1]] = element[2]\n",
    "                else:\n",
    "                    if element[1] in part_dif.iloc[i,5]:\n",
    "                        if element[2] == '0.00000000':\n",
    "                            del part_dif.iloc[i,5][element[1]]\n",
    "                        else:\n",
    "                            part_dif.iat[i,5][element[1]] = element[2]\n",
    "                    elif element[2] != '0.00000000':\n",
    "                        part_dif.iat[i,5][element[1]] = element[2]          \n",
    "        elif part_dif.iloc[i,0] != \"-\":\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i,0].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i,1].copy()\n",
    "            current = part_dif.iloc[0,3]\n",
    "        else:\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i-1,4].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "    \n",
    "    lrow = part_dif.iloc[-1,:]\n",
    "    return part_dif.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Huobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "def create_Huobi(part, part_dif):\n",
    "    # First updates\n",
    "    part.loc[:,\"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict(ast.literal_eval(x)) if x != \"-\" else x)\n",
    "    part.loc[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict(ast.literal_eval(x)) if x != \"-\" else x)  \n",
    "    part = part.rename(columns = {\"Asks\": \"U_Asks\", \"Bids\": \"U_Bids\"})\n",
    "    return part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 Kraken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp in message\n",
    "# Careful with \"r\" for republished updates!\n",
    "def create_Kraken(part, part_dif, last_row):\n",
    "    part.at[:,\"Host\"] = \"Snapshot\"\n",
    "    \n",
    "    part_dif = pd.concat([part[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]], part_dif[[\"Asks\",\"Bids\", \"Host\", \"Pair\"]]]).sort_index()\n",
    "    part.loc[:,\"Asks\"] = part.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part.loc[:,\"Bids\"] = part.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)  \n",
    "    part_dif.loc[:,\"Asks\"] =  part_dif.loc[:,\"Asks\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)\n",
    "    part_dif.loc[:,\"Bids\"] =  part_dif.loc[:,\"Bids\"].apply(lambda x: dict([[i[0],i[1]] for i in ast.literal_eval(x)]) if x != \"-\" else x)        # Rebuild order book for each timestemp we have\n",
    "\n",
    "    part_dif = part_dif.loc[part_dif[part_dif[\"Host\"] == \"Snapshot\"].index[0]::,:]\n",
    "\n",
    "    part_dif[\"U_Asks\"] = \"\"\n",
    "    part_dif[\"U_Bids\"] = \"\"\n",
    "    part_dif.iat[0,4] = part.iloc[0,0].copy()\n",
    "    part_dif.iat[0,5] = part.iloc[0,1].copy()\n",
    "    if type(last_row) != type(None):\n",
    "        part_dif = pd.concat([pd.DataFrame(last_row).T, part_dif])   \n",
    "\n",
    "    current = part_dif.iloc[0,3]\n",
    "    for i in range(1,len(part_dif)):\n",
    "        if part_dif.iloc[i,2] != \"Snapshot\" and part_dif.iloc[i-1,3] == current:\n",
    "            for j in range(2):\n",
    "                part_dif.iat[i,4+j] = part_dif.iloc[i-1,4+j].copy()\n",
    "                if part_dif.iloc[i,j] != \"-\":\n",
    "                    for key in part_dif.iloc[i,j].keys():\n",
    "                        if  key in part_dif.iloc[i,4+j]:\n",
    "                            if float(part_dif.iloc[i,j][key][0]) == 0.0:\n",
    "                                del part_dif.iloc[i,4+j][key]\n",
    "                            else:\n",
    "                                part_dif.iat[i,4+j][key] = part_dif.iloc[i,j][key][0]\n",
    "                        else:\n",
    "                            part_dif.iat[i,4+j][key] = part_dif.iloc[i,j][key][0]        \n",
    "        elif part_dif.iloc[i,2] == \"Snapshot\":\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i,0].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i,1].copy()\n",
    "            current = part_dif.iloc[0,3]\n",
    "        else:\n",
    "            part_dif.iat[i,4] = part_dif.iloc[i-1,4].copy()\n",
    "            part_dif.iat[i,5] = part_dif.iloc[i-1,5].copy()\n",
    "       \n",
    "    lrow = part_dif.iloc[-1,:]\n",
    "    \n",
    "    return part_dif.iloc[1::,:], lrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(ex, part_dif):\n",
    "    # Compute orderbook price to add to the main dataset\n",
    "    first = 0\n",
    "    second = 0\n",
    "    part_dif[\"OrderBookPrice \" + ex] = \"\"\n",
    "    indb = part_dif.columns.get_loc(\"U_Bids\")\n",
    "    inda = part_dif.columns.get_loc(\"U_Asks\")\n",
    "    indf = part_dif.columns.get_loc(\"OrderBookPrice \" + ex) \n",
    "    for i in range(len(part_dif)):\n",
    "\n",
    "        values = sorted(part_dif.iloc[i,inda].keys())\n",
    "        values2 = sorted(part_dif.iloc[i,indb].keys(), reverse = True)\n",
    "        for j in values:\n",
    "            if float(part_dif.iloc[i,inda][j]) > 0:\n",
    "                first = float(j)\n",
    "                break\n",
    "        for j in values2:\n",
    "            if float(part_dif.iloc[i,indb][j]) > 0:\n",
    "                second = float(j)\n",
    "                break\n",
    "        part_dif.iat[i,indf] = (first + second) / 2\n",
    "        \n",
    "        part_dif.iat[i,indb] = { your_key: part_dif.iat[i,indb][your_key] for your_key in values2[0:40] }\n",
    "        part_dif.iat[i,inda] = { your_key: part_dif.iat[i,inda][your_key] for your_key in values[0:40] }\n",
    "\n",
    "    separate = part_dif.iloc[:,[inda,indb]].apply(lambda x: pd.Series([item for sublist in  \n",
    "                                                 [[float(list(x[0].keys())[i]), float(list(x[0].values())[i]),\n",
    "                                                   float(list(x[1].keys())[i]), float(list(x[1].values())[i])] for i in range(40)] \n",
    "                                                  for item in sublist]), axis = 1).rename(columns = dict(\n",
    "                             [[i,\"Price_A_\" + str(int(i/4)) + \"_\" + ex] if i % 4 == 0 else \n",
    "                             [i,\"Price_B_\" + str(int(i/4)) + \"_\" + ex] if i % 4 == 2  else\n",
    "                             [i,\"Quan_A_\" + str(int(i/4)) + \"_\" + ex] if i % 4 == 1 else\n",
    "                             [i,\"Quan_B_\" + str(int(i/4)) + \"_\" + ex] for i in range(160)]))\n",
    "#     [print(col) for col in separate.columns]\n",
    "    part_dif = pd.concat([part_dif.drop([\"U_Asks\", \"U_Bids\"], axis = 1), separate], axis = 1)\n",
    "    \n",
    "    return part_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_main(ex, df2, part_join):\n",
    "    filter_col = [col for col in part_join if (col.startswith('Price_') or col.startswith('Quan_') or col.startswith('OrderBook'))]\n",
    "    df2 = pd.merge(df2,part_join[filter_col], how='outer', left_index=True, right_index=True)\n",
    "    filter_col = [col for col in df2 if (col.startswith('Last') or col.startswith('Order')\n",
    "                                         or col.startswith(\"Price_\") or col.startswith(\"Quan_\"))]\n",
    "    df2.update(df2[filter_col].ffill())\n",
    "    filter_col = [col for col in df2 if (col.startswith('Q ') or col.startswith('Q_'))]\n",
    "    df2.update(df2[filter_col].fillna(0))\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_OrderBook(ex, df2, part,part_dif, last_row = None):\n",
    "    if ex == \"Bitstamp\":\n",
    "        part_dif = create_Bitstamp(part, part_dif)\n",
    "    elif ex == \"Bitfinex\":\n",
    "        part_dif, last_row = create_Bitfinex(part, part_dif, last_row)\n",
    "    elif ex == \"Binance\":\n",
    "        part_dif, last_row = create_Binance(part, part_dif, last_row)\n",
    "    elif ex == \"Huobi\":\n",
    "        part_dif = create_Huobi(part, part_dif)\n",
    "    elif ex == \"Kraken\":\n",
    "        part_dif, last_row = create_Kraken(part, part_dif, last_row)\n",
    "    elif ex == \"Coinbase\":\n",
    "        part_dif, last_row = create_Coinbase(part, part_dif, last_row)\n",
    "    elif ex == \"Bithumb\":\n",
    "        part_dif, last_row = create_Bithumb(part, part_dif, last_row)\n",
    "    \n",
    "    part_dif = compute_stats(ex, part_dif)\n",
    "    df2 = join_main(ex, df2,part_dif)\n",
    "    \n",
    "    return df2, last_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_core(df, pair, exchanges, lrow):\n",
    "    df1 = df[df[\"Pair\"] == pair]\n",
    "    \n",
    "    df2 = df1[df1[\"Host\"] == exchanges[0]].groupby(pd.Grouper(freq = \"ms\")).agg({\"Price\":[np.mean],\"Q\":[np.sum]}).fillna(method = \"ffill\")\n",
    "    df2.columns = df2.columns.droplevel(level = 0)\n",
    "    df2 = df2.rename(columns = {\"mean\": \"LastTrade_\" + exchanges[0], \"sum\": \"Q_\" + exchanges[0]})\n",
    "    for i in exchanges[1::]:\n",
    "        dft = df1[df1[\"Host\"] == i].groupby(pd.Grouper(freq = \"ms\")).agg({\"Price\":[np.mean],\"Q\":[np.sum]}).fillna(method = \"ffill\")\n",
    "        df2[[\"LastTrade_\" + i, \"Q_\"+i]] = dft[[\"Price\", \"Q\"]]\n",
    "        \n",
    "    if type(lrow) != type(None):\n",
    "        df2 = pd.concat([pd.DataFrame(lrow).T, df2])\n",
    "        \n",
    "    col1 = [col for col in df2 if (col.startswith('Last'))]\n",
    "    df2.update(df2[col1].ffill().bfill())\n",
    "    col2 = [col for col in df2 if (col.startswith('Q'))]\n",
    "    df2.update(df2[col2].fillna(0))\n",
    "    \n",
    "    df2.drop_duplicates(keep = \"first\", inplace = True)\n",
    "    \n",
    "    return df2.iloc[1::,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(date_init, date_end):\n",
    "    \n",
    "    points = client.query(\"SELECT Host, Pair, Price, Q FROM Price WHERE time >= '\"+ str(date_init) +\"' AND time < '\" + str(date_end) + \"'\")\n",
    "    points_book = client.query(\"SELECT Asks,Bids, Host, LastUpdateID, Pair FROM Book WHERE time >= '\"+ str(date_init) +\"' AND time < '\" + str(date_end) + \"' AND LastUpdateID != ''\")\n",
    "    points_dif = client.query(\"SELECT Asks, Bids, Host, Pair, Seg FROM difBook WHERE time >= '\"+ str(date_init) +\"' AND time < '\" + str(date_end) + \"'\")\n",
    "\n",
    "    last_trades = points[\"Price\"]\n",
    "    book_snap = points_book[\"Book\"]\n",
    "    book_dif = points_dif[\"difBook\"]\n",
    "    \n",
    "    return last_trades,book_snap,book_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_metrics_rmdups(df):\n",
    "    # Compute the max difference and other metrics and remove duplicates\n",
    "    filter_col = [col for col in df if col.startswith('Order')]\n",
    "    df[\"Maximum\"] = df[filter_col].apply(lambda row: np.max(row) - np.min(row), axis=1)\n",
    "\n",
    "    df[\"Average\"] = df[filter_col].apply(lambda row: np.mean(row)*0.002, axis=1)\n",
    "\n",
    "    df[\"MaxExch\"] = df[filter_col].fillna(0).apply(lambda row: sorted([row.idxmax()[15::], row.idxmin()[15::]])[0], axis=1)\n",
    "    df[\"MinExch\"] = df[filter_col].fillna(0).apply(lambda row: sorted([row.idxmax()[15::], row.idxmin()[15::]])[1], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_labels(df):\n",
    "    # If there are more than 10 consecutive >avg*0,002 then 1 otherwise 0.\n",
    "    df[\"Index_Not\"] = df[[\"Average\",\"Maximum\"]].apply(lambda x: 1 if x[0]<x[1] else 0, axis = 1)\n",
    "    df[\"Index\"] = df[\"Index_Not\"]\n",
    "    for i in range(len(df)-500):\n",
    "        array = np.diff(df[\"Index_Not\"][i:i+500])\n",
    "        starts, = np.where(array>0) + np.ones(len(np.where(array>0)))\n",
    "        ends, = np.where(array<0) + np.ones(len(np.where(array<0)))\n",
    "        if len(starts) != len(ends):\n",
    "            if len(ends) == 0:\n",
    "                ends = np.append(ends,len(array))\n",
    "            elif len(starts) == 0:\n",
    "                starts = np.append(0,starts)\n",
    "            elif starts[0] > ends[0]:\n",
    "                starts = np.append(0,starts)\n",
    "            else:\n",
    "                ends = np.append(ends,len(array))\n",
    "\n",
    "        if len(ends) != 0 and len(starts) != 0:\n",
    "            if np.max(ends - starts) >= 10:\n",
    "                df.iat[i, -1] = 1\n",
    "            else:\n",
    "                df.iat[i, -1] = 0\n",
    "        elif array[[0]] == 1:\n",
    "            df.iat[i, -1] = 1\n",
    "        else:\n",
    "            df.iat[i, -1] = 0\n",
    "    df = df.drop(\"Index_Not\", axis = 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Final dataframe process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DataFrameClient('localhost', 8086, 'root', 'root')\n",
    "client.switch_database(\"SecondM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Book'}, {'name': 'Price'}, {'name': 'difBook'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_list_measurements()\n",
    "client.drop_measurement(\"Dataset\")\n",
    "client.get_list_measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Main core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_number = 1\n",
    "interval = 15\n",
    "\n",
    "start_time = datetime.strptime(\"2020-05-21 16:02:30.000000\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "endtime = start_time + timedelta(minutes = interval)\n",
    "\n",
    "finish_process_time = start_time + timedelta(minutes = 14)\n",
    "\n",
    "pairs = [\"ethbtc\"]\n",
    "# pairs = [\"ethbtc\", \"btcusd\", \"ethusd\", \"xtzusd\"]\n",
    "\n",
    "exchanges_snap = [\"Binance\", \"Bitfinex\", \"Bithumb\", \"Bitstamp\", \"Coinbase\", \"Huobi\", \"Kraken\"]\n",
    "\n",
    "analysis_start = None\n",
    "\n",
    "max_dif = 0\n",
    "time_max_dif = 0\n",
    "\n",
    "last_rows = [None]*9\n",
    "\n",
    "last_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binance\n",
      "Bitfinex\n",
      "Bithumb\n",
      "Bitstamp\n",
      "Coinbase\n",
      "Huobi\n",
      "Kraken\n",
      "2020-05-28 23:08:46.828219: Writing to dataframe with start time 2020-05-21 16:02:30\n",
      "Total time: 1970-01-01 01:06:58.890615\n"
     ]
    }
   ],
   "source": [
    "# Check disconnections!\n",
    "\n",
    "# TO DO, check disconnections for start and end times. Add field in new table for sequence.\n",
    "# REMMEMBER TO DELETE THE FIRST ROW BEFORE REMOVING DUPLICATES\n",
    "\n",
    "# For every hour\n",
    "start = time.time()\n",
    "\n",
    "while start_time <= finish_process_time:\n",
    "        \n",
    "    last_trades, book_snap, book_dif = query(start_time, endtime)\n",
    "\n",
    "    # We create the base dataset\n",
    "    core_df = create_core(last_trades, pairs[0],exchanges_snap,last_rows[7])\n",
    "    \n",
    "    last_rows[7] = core_df.iloc[-1,:]\n",
    "    \n",
    "    # We add orderbook data\n",
    "    for k in range(len(exchanges_snap)):\n",
    "        print(exchanges_snap[k])\n",
    "        part = book_snap[book_snap[\"Host\"] == exchanges_snap[k]]\n",
    "        part = part[part[\"Pair\"].str.contains(pairs[0])].loc[:, part.columns != 'Batch_ID']\n",
    "        part = part[part[\"Pair\"] == pairs[0]]\n",
    "        \n",
    "        part_dif = book_dif[book_dif[\"Host\"] == exchanges_snap[k]]\n",
    "        part_dif = part_dif[part_dif[\"Pair\"].str.contains(pairs[0])].loc[:, part_dif.columns != 'Batch_ID']\n",
    "        part_dif = part_dif[part_dif[\"Pair\"] == pairs[0]]\n",
    "\n",
    "        [core_df, last_rows[k]] = create_OrderBook(exchanges_snap[k], core_df, part, part_dif,last_rows[k])\n",
    "\n",
    "\n",
    "    # Compute last metrics\n",
    "    core_df = last_metrics_rmdups(core_df)\n",
    "    \n",
    "    # Check where the maximum difference is\n",
    "    if np.max(core_df[\"Maximum\"]) > max_dif:\n",
    "        max_dif = np.max(core_df[\"Maximum\"]) \n",
    "        time_max_dif = start_time\n",
    "    \n",
    "    # Create input and calculate labels\n",
    "    \n",
    "    columns = [col for col in core_df if (col.startswith('Price_') or col.startswith('Quan_'))]\n",
    "    columns2 = [col for col in core_df if (col.startswith(\"Last\")  or col.startswith(\"Q_\"))]\n",
    "    matrixes = np.concatenate([np.asarray(core_df[columns2]).reshape(len(core_df), 2,7).transpose(0, 2, 1),\n",
    "                   np.asarray(core_df[columns]).reshape(len(core_df), 7,160)], axis = 2)\n",
    "    core_df[\"Input\"] = [matrixes[i].reshape(7,162) for i in range(len(core_df))]\n",
    "    core_df = core_df.drop(columns + columns2, axis = 1)\n",
    "    \n",
    "    if type(last_rows[8]) != type(None):\n",
    "        core_df = pd.concat([last_rows[8], core_df])\n",
    "        \n",
    "    last_rows[8] =  core_df.iloc[-500::,:]   \n",
    "\n",
    "    # I think because of the notation 1E-5 influxdb read things as string so we convert everything to float\n",
    "    \n",
    "    print(str(datetime.fromtimestamp(time.time())) + \": Writing to dataframe with start time \" + str(start_time))\n",
    "    filter_col = [col for col in core_df if (col.startswith('Maximum') or col.startswith('Ave') or col.startswith('Order'))]\n",
    "    for col in filter_col:\n",
    "        core_df.at[:,col] = core_df.loc[:,col].astype(float)\n",
    "        \n",
    "    # Create Labels\n",
    "    core_df = calculate_labels(core_df)\n",
    "    \n",
    "    # Write to database\n",
    "        \n",
    "    client.write_points(core_df.iloc[0:-500,:].assign(Batch_ID = [i for i in range(last_batch, last_batch+len(core_df)-500)]),\n",
    "                        \"Dataset\",time_precision = \"n\", database = \"SecondM\", batch_size = 100)\n",
    "    last_batch = last_batch+len(core_df)-500\n",
    "    \n",
    "    # We set up the interval times, and gather the first timestamp that we have data\n",
    "    start_time = endtime\n",
    "    endtime = start_time + timedelta(minutes = interval)\n",
    "    if core_df.isna().sum().sum() == 0 and type(analysis_start) != type(None):\n",
    "        analysis_start = endtime\n",
    "    \n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "print(\"Total time: \" + str(datetime.fromtimestamp(end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderBookPrice Binance</th>\n",
       "      <th>OrderBookPrice Bitfinex</th>\n",
       "      <th>OrderBookPrice Bithumb</th>\n",
       "      <th>OrderBookPrice Bitstamp</th>\n",
       "      <th>OrderBookPrice Coinbase</th>\n",
       "      <th>OrderBookPrice Huobi</th>\n",
       "      <th>OrderBookPrice Kraken</th>\n",
       "      <th>Maximum</th>\n",
       "      <th>Average</th>\n",
       "      <th>MaxExch</th>\n",
       "      <th>MinExch</th>\n",
       "      <th>Input</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:04:47.428157952+00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Binance</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:04:47.693092096+00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Binance</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:04:48.021548032+00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Binance</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:04:48.285448960+00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Binance</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:04:48.570487040+00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Binance</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:17:29.814000+00:00</th>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.02208</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Bithumb</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[0.022093500000000002, 0.0, 0.022094, 0.013, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:17:29.827346944+00:00</th>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.02208</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Bithumb</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[0.022093500000000002, 0.0, 0.022094, 0.013, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:17:29.860000+00:00</th>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.02208</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Bithumb</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[0.022099, 0.0, 0.022094, 0.013, 0.022092, 1....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:17:29.861000+00:00</th>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.02208</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Bithumb</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[0.022099, 0.0, 0.022094, 0.013, 0.022092, 1....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21 16:17:29.964000+00:00</th>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.02208</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>Bithumb</td>\n",
       "      <td>Bitstamp</td>\n",
       "      <td>[[0.02209825, 0.0, 0.022094, 0.013, 0.022092, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14142 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     OrderBookPrice Binance  \\\n",
       "2020-05-21 16:04:47.428157952+00:00                     NaN   \n",
       "2020-05-21 16:04:47.693092096+00:00                     NaN   \n",
       "2020-05-21 16:04:48.021548032+00:00                     NaN   \n",
       "2020-05-21 16:04:48.285448960+00:00                     NaN   \n",
       "2020-05-21 16:04:48.570487040+00:00                     NaN   \n",
       "...                                                     ...   \n",
       "2020-05-21 16:17:29.814000+00:00                   0.022093   \n",
       "2020-05-21 16:17:29.827346944+00:00                0.022093   \n",
       "2020-05-21 16:17:29.860000+00:00                   0.022093   \n",
       "2020-05-21 16:17:29.861000+00:00                   0.022093   \n",
       "2020-05-21 16:17:29.964000+00:00                   0.022093   \n",
       "\n",
       "                                     OrderBookPrice Bitfinex  \\\n",
       "2020-05-21 16:04:47.428157952+00:00                      NaN   \n",
       "2020-05-21 16:04:47.693092096+00:00                      NaN   \n",
       "2020-05-21 16:04:48.021548032+00:00                      NaN   \n",
       "2020-05-21 16:04:48.285448960+00:00                      NaN   \n",
       "2020-05-21 16:04:48.570487040+00:00                      NaN   \n",
       "...                                                      ...   \n",
       "2020-05-21 16:17:29.814000+00:00                    0.022085   \n",
       "2020-05-21 16:17:29.827346944+00:00                 0.022085   \n",
       "2020-05-21 16:17:29.860000+00:00                    0.022085   \n",
       "2020-05-21 16:17:29.861000+00:00                    0.022085   \n",
       "2020-05-21 16:17:29.964000+00:00                    0.022085   \n",
       "\n",
       "                                     OrderBookPrice Bithumb  \\\n",
       "2020-05-21 16:04:47.428157952+00:00                     NaN   \n",
       "2020-05-21 16:04:47.693092096+00:00                     NaN   \n",
       "2020-05-21 16:04:48.021548032+00:00                     NaN   \n",
       "2020-05-21 16:04:48.285448960+00:00                     NaN   \n",
       "2020-05-21 16:04:48.570487040+00:00                     NaN   \n",
       "...                                                     ...   \n",
       "2020-05-21 16:17:29.814000+00:00                   0.022074   \n",
       "2020-05-21 16:17:29.827346944+00:00                0.022074   \n",
       "2020-05-21 16:17:29.860000+00:00                   0.022074   \n",
       "2020-05-21 16:17:29.861000+00:00                   0.022074   \n",
       "2020-05-21 16:17:29.964000+00:00                   0.022074   \n",
       "\n",
       "                                     OrderBookPrice Bitstamp  \\\n",
       "2020-05-21 16:04:47.428157952+00:00                 0.022081   \n",
       "2020-05-21 16:04:47.693092096+00:00                 0.022081   \n",
       "2020-05-21 16:04:48.021548032+00:00                 0.022081   \n",
       "2020-05-21 16:04:48.285448960+00:00                 0.022081   \n",
       "2020-05-21 16:04:48.570487040+00:00                 0.022081   \n",
       "...                                                      ...   \n",
       "2020-05-21 16:17:29.814000+00:00                    0.022097   \n",
       "2020-05-21 16:17:29.827346944+00:00                 0.022097   \n",
       "2020-05-21 16:17:29.860000+00:00                    0.022097   \n",
       "2020-05-21 16:17:29.861000+00:00                    0.022097   \n",
       "2020-05-21 16:17:29.964000+00:00                    0.022097   \n",
       "\n",
       "                                     OrderBookPrice Coinbase  \\\n",
       "2020-05-21 16:04:47.428157952+00:00                      NaN   \n",
       "2020-05-21 16:04:47.693092096+00:00                      NaN   \n",
       "2020-05-21 16:04:48.021548032+00:00                      NaN   \n",
       "2020-05-21 16:04:48.285448960+00:00                      NaN   \n",
       "2020-05-21 16:04:48.570487040+00:00                      NaN   \n",
       "...                                                      ...   \n",
       "2020-05-21 16:17:29.814000+00:00                     0.02208   \n",
       "2020-05-21 16:17:29.827346944+00:00                  0.02208   \n",
       "2020-05-21 16:17:29.860000+00:00                     0.02208   \n",
       "2020-05-21 16:17:29.861000+00:00                     0.02208   \n",
       "2020-05-21 16:17:29.964000+00:00                     0.02208   \n",
       "\n",
       "                                     OrderBookPrice Huobi  \\\n",
       "2020-05-21 16:04:47.428157952+00:00                   NaN   \n",
       "2020-05-21 16:04:47.693092096+00:00                   NaN   \n",
       "2020-05-21 16:04:48.021548032+00:00                   NaN   \n",
       "2020-05-21 16:04:48.285448960+00:00                   NaN   \n",
       "2020-05-21 16:04:48.570487040+00:00                   NaN   \n",
       "...                                                   ...   \n",
       "2020-05-21 16:17:29.814000+00:00                 0.022089   \n",
       "2020-05-21 16:17:29.827346944+00:00              0.022089   \n",
       "2020-05-21 16:17:29.860000+00:00                 0.022089   \n",
       "2020-05-21 16:17:29.861000+00:00                 0.022089   \n",
       "2020-05-21 16:17:29.964000+00:00                 0.022089   \n",
       "\n",
       "                                     OrderBookPrice Kraken   Maximum  \\\n",
       "2020-05-21 16:04:47.428157952+00:00                    NaN  0.000000   \n",
       "2020-05-21 16:04:47.693092096+00:00                    NaN  0.000000   \n",
       "2020-05-21 16:04:48.021548032+00:00                    NaN  0.000000   \n",
       "2020-05-21 16:04:48.285448960+00:00                    NaN  0.000000   \n",
       "2020-05-21 16:04:48.570487040+00:00                    NaN  0.000000   \n",
       "...                                                    ...       ...   \n",
       "2020-05-21 16:17:29.814000+00:00                  0.022095  0.000023   \n",
       "2020-05-21 16:17:29.827346944+00:00               0.022095  0.000023   \n",
       "2020-05-21 16:17:29.860000+00:00                  0.022095  0.000023   \n",
       "2020-05-21 16:17:29.861000+00:00                  0.022095  0.000023   \n",
       "2020-05-21 16:17:29.964000+00:00                  0.022095  0.000023   \n",
       "\n",
       "                                      Average  MaxExch   MinExch  \\\n",
       "2020-05-21 16:04:47.428157952+00:00  0.000044  Binance  Bitstamp   \n",
       "2020-05-21 16:04:47.693092096+00:00  0.000044  Binance  Bitstamp   \n",
       "2020-05-21 16:04:48.021548032+00:00  0.000044  Binance  Bitstamp   \n",
       "2020-05-21 16:04:48.285448960+00:00  0.000044  Binance  Bitstamp   \n",
       "2020-05-21 16:04:48.570487040+00:00  0.000044  Binance  Bitstamp   \n",
       "...                                       ...      ...       ...   \n",
       "2020-05-21 16:17:29.814000+00:00     0.000044  Bithumb  Bitstamp   \n",
       "2020-05-21 16:17:29.827346944+00:00  0.000044  Bithumb  Bitstamp   \n",
       "2020-05-21 16:17:29.860000+00:00     0.000044  Bithumb  Bitstamp   \n",
       "2020-05-21 16:17:29.861000+00:00     0.000044  Bithumb  Bitstamp   \n",
       "2020-05-21 16:17:29.964000+00:00     0.000044  Bithumb  Bitstamp   \n",
       "\n",
       "                                                                                 Input  \\\n",
       "2020-05-21 16:04:47.428157952+00:00  [[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...   \n",
       "2020-05-21 16:04:47.693092096+00:00  [[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...   \n",
       "2020-05-21 16:04:48.021548032+00:00  [[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...   \n",
       "2020-05-21 16:04:48.285448960+00:00  [[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...   \n",
       "2020-05-21 16:04:48.570487040+00:00  [[nan, 0.0, nan, nan, nan, nan, nan, nan, nan,...   \n",
       "...                                                                                ...   \n",
       "2020-05-21 16:17:29.814000+00:00     [[0.022093500000000002, 0.0, 0.022094, 0.013, ...   \n",
       "2020-05-21 16:17:29.827346944+00:00  [[0.022093500000000002, 0.0, 0.022094, 0.013, ...   \n",
       "2020-05-21 16:17:29.860000+00:00     [[0.022099, 0.0, 0.022094, 0.013, 0.022092, 1....   \n",
       "2020-05-21 16:17:29.861000+00:00     [[0.022099, 0.0, 0.022094, 0.013, 0.022092, 1....   \n",
       "2020-05-21 16:17:29.964000+00:00     [[0.02209825, 0.0, 0.022094, 0.013, 0.022092, ...   \n",
       "\n",
       "                                     Index  \n",
       "2020-05-21 16:04:47.428157952+00:00      0  \n",
       "2020-05-21 16:04:47.693092096+00:00      0  \n",
       "2020-05-21 16:04:48.021548032+00:00      0  \n",
       "2020-05-21 16:04:48.285448960+00:00      0  \n",
       "2020-05-21 16:04:48.570487040+00:00      0  \n",
       "...                                    ...  \n",
       "2020-05-21 16:17:29.814000+00:00         0  \n",
       "2020-05-21 16:17:29.827346944+00:00      0  \n",
       "2020-05-21 16:17:29.860000+00:00         0  \n",
       "2020-05-21 16:17:29.861000+00:00         0  \n",
       "2020-05-21 16:17:29.964000+00:00         0  \n",
       "\n",
       "[14142 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
